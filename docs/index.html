<!DOCTYPE html>
<html lang="en">
  <head>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link rel="stylesheet" href="style.css" />
    <title>Vision-based navigation using Deep Reinforcement Learning</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="This webpage accompanies the paper Vision-based navigation using Deep Reinforcement Learning and provides a link to the official code implementation called A2CAT-VN." />
    <meta name="author" content="Jonáš Kulhánek" />
    <meta name="keywords" content="deep reinforcement learning,visual navigation,deep learning,paper,code,arxiv" />
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&display=swap" rel="stylesheet">
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-133355320-1"></script><script>window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());gtag('config', 'UA-133355320-1');</script>
  </head>
  <body>
    <header>
      <h1>
        <span class="title-small">Vision-based navigation using Deep Reinforcement Learning</span>
      </h1>
    </header>
    <div class="authors">
      <div class="author">
        <span class="author-name">
          <a href="https://jkulhanek.github.io/">Jonáš Kulhánek</a>
        </span>
        <span class="author-affiliation">Delft University of Technology</span>
      </div>
      <div class="author">
        <span class="author-name">
          <a href="http://people.ciirc.cvut.cz/~derneeri/">Erik Derner</a>
        </span>
        <span class="author-affiliation">Czech Technical University in Prague</span>
      </div>
      <div class="author">
        <span class="author-name">
          <a href="https://scholar.google.ca/citations?user=AM8Se74AAAAJ&hl=en">Tim de Bruin</a>
        </span>
        <span class="author-affiliation">Delft University of Technology</span>
      </div>
      <div class="author">
        <span class="author-name">
          <a href="http://www.robertbabuska.com/">Robert Babuška</a>
        </span>
        <span class="author-affiliation">Delft University of Technology</span>
      </div>
    </div>
    <div class="links">
      <div class="link link-paper">
        <a href="https://arxiv.org/pdf/1908.03627.pdf">Paper</a>
      </div>
      <div class="link link-github">
        <a href="https://github.com/jkulhanek/a2cat-vn">Code</a>
      </div>
    </div>
    <div class="video">
      <video width="320" height="240" loop autoplay muted>
        <source src="https://data.ciirc.cvut.cz/public/projects/2019VisionBasedNavigation/videos/dmhouse.mp4" type="video/mp4">
      </video>
    </div>
    <div class="video-caption" style="display: flex;width: 100%;flex-direction: row;align-items: stretch;">
      <span style="flex: 1;text-align: center;">observation</span>
      <span style="flex: 1;text-align: center;">target</span>
    </div>

    <section class="abstract">
      <h2>Abstract</h2>
      <p>
      Deep reinforcement learning (RL) has been successfully applied to a variety of game-like environments. However, the application of deep RL to visual navigation with
realistic environments is a challenging task. We propose a novel learning architecture capable of navigating an agent, e.g. a mobile robot, to a target given by an image. To
achieve this, we have extended the batched A2C algorithm with auxiliary tasks designed to improve visual navigation performance.</p>
      <p>We propose three additional auxiliary tasks: predicting the segmentation of the observation image and of the target image and predicting the depth-map.
<img src="resources/overview.svg" alt="ViewFormer architecture overview" style="width: 100%; max-width:448px; margin: 0px auto; display: block;" />
These tasks enable the use of supervised learning to pre-train a large part of the network and to reduce the number of training steps substantially. The training performance has been further improved by increasing the environment complexity gradually over time. An efficient neural network structure is proposed, which is capable of learning for multiple targets in multiple environments. Our method navigates in continuous state spaces and on the AI2-THOR environment simulator outperforms state-of-the-art goal-oriented visual navigation methods from the literature.
    </section>
    <section class="results-dmhouse">
      <h2>SUNCG dataset results</h2>
      We compared our method (A2CAT-VN) with the batched A2C extended with the original two UNREAL auxiliary tasks on the SUNCG dataset [<a class="citation" href="#ref-song2017semantic">1</a>]. Our algorithm A2CAT-VN converged much faster with the additional auxiliary tasks for visual navigation enabled, reaching the average episode length of 200 in roughly 3 &middot; 10<sup>6</sup> frames whereas without the additional tasks the training took roughly 8 &middot; 10<sup>6</sup> frames to get to the same level.
 The following plot shows the average episode length during training. 
      <div class="figure">
        <img src="resources/suncg.png" alt="SUNCG training comparison" />
      </div>
    </section>
    <section class="results-ai2thor">
      <h2>AI2-THOR environment results</h2>
      We have trained our algorithm on four environments from AI2-THOR simulator [<a class="citation" href="#ref-kolve2017ai2">2</a>] with multiple targets. Our method is compared to Zhu's method [<a class="citation" href="#ref-zhu2017target">3</a>]. The environments we chose for this experiment were bigger and more difficult to navigate than those used in [<a class="citation" href="#ref-zhu2017target">3</a>], but came from the same AI2-THOR simulator. The training with our algorithm took roughly one day, while it took three days to train the network by using the algorithm described in [<a class="citation" href="#ref-zhu2017target">3</a>]. The following plot compares our model <strong>(A2CAT-VN)</strong> with Zhu's work [<a class="citation" href="#ref-zhu2017target">3</a>] <strong>(ICRA2017)</strong> and shows the average episode length during training.
      <div class="figure">
        <img src="resources/aithor.png" alt="AI2THOR training comparison" />
      </div>
    </section>
    <section class="references">
      <h2>References</h2>
      <div id="ref-song2017semantic">
        [1] Shuran Song, Fisher Yu, Andy Zeng, Angel X. Chang, Manolis Savva, and Thomas Funkhouser. <i>Semantic scene completion from a single depth image.</i> In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1746-1754, 2017.
      </div>
      <div id="ref-kolve2017ai2">
        [2] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. <i>AI2-THOR: An interactive 3D environment for visual AI.</i> arXiv preprint arXiv:1712.05474, 2017.
      </div>
      <div id="ref-zhu2017target">
        [3] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J. Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. <i>Target-driven visual navigation in indoor scenes using deep reinforcement learning.</i> In 2017 IEEE international conference on robotics and automation (ICRA), pages 3357-3364, 2017.
      </div>
    </section>
    <section class="citation">
      <h2>Citation</h2>
      <span>Please use the following citation:</span>
      <pre>
@inproceedings{kulhanek2019vision,
  title={Vision-based navigation using deep reinforcement learning},
  author={Kulh{\'a}nek, Jon{\'a}{\v{s}} and Derner, Erik and De Bruin, Tim and Babu{\v{s}}ka, Robert},
  booktitle={2019 European Conference on Mobile Robots (ECMR)},
  pages={1--8},
  year={2019},
  organization={IEEE}
}
</pre>
    </section>
<a href="https://github.com/jkulhanek/a2cat-vn/" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
  </body>
</html>
